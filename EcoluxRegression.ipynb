{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sMISnNMasvPM",
   "metadata": {
    "id": "sMISnNMasvPM"
   },
   "source": [
    "# Upload Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nSRayliLXAiK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "nSRayliLXAiK",
    "outputId": "10e2cdd4-aa8a-4dfa-da83-d15ef2b63479"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-61db8c0c-352d-42c4-b75d-f297b4b1b23a\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-61db8c0c-352d-42c4-b75d-f297b4b1b23a\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving allHousesUnfiltered.csv to allHousesUnfiltered.csv\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "S4upR4XfWzIq",
   "metadata": {
    "id": "S4upR4XfWzIq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "house_df = pd.read_csv(io.StringIO(uploaded['allHouses.csv'].decode('utf-8')))\n",
    "# house_df_unfiltered = pd.read_csv(io.StringIO(uploaded['allHousesUnfiltered.csv'].decode('utf-8')))\n",
    "\n",
    "print(house_df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "GZy-V-zQyn9Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZy-V-zQyn9Y",
    "outputId": "f83c7bdc-94ae-4bc2-87d0-ca7dcba7d9be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HouseNum', 'TimeSin', 'TimeCos', 'DayNumSin', 'DayNumCos', 'MonthSin', 'MonthCos', 'RealTemp', 'ApparTemp', 'Humid', 'Wmo_0-9', 'Wmo_10-19', 'Wmo_20-29', 'Wmo_30-39', 'Wmo_40-49', 'Wmo_50-59', 'Wmo_60-69', 'Wmo_70-79', 'Wmo_80-89', 'Wmo_90-99', 'YearBuilt__under_1899', 'YearBuilt_1900-1909', 'YearBuilt_1910-1919', 'YearBuilt_1920-1929', 'YearBuilt_1930-1939', 'YearBuilt_1940-1949', 'YearBuilt_1950-1959', 'YearBuilt_1960-1969', 'YearBuilt_1970-1979', 'YearBuilt_1980-1989', 'YearBuilt_1990-1999', 'YearBuilt_2000-2009', 'YearBuilt_2010-2019', 'YearBuilt_2020-2029', 'Type_BUNGALOW', 'Type_COTTAGE', 'Type_DETACHED', 'Type_END OF TERRACE', 'Type_FLAT', 'Type_SEMI-DETACHED', 'Type_MID-TERRACE', 'NumRooms', 'NumOccupants', 'Total', 'AlwaysOn', 'Intermit', 'HVAC']\n",
      "['HouseNum', 'TimeSin', 'TimeCos', 'DayNumSin', 'DayNumCos', 'MonthSin', 'MonthCos', 'RealTemp', 'ApparTemp', 'Humid', 'Wmo_0-9', 'Wmo_10-19', 'Wmo_20-29', 'Wmo_30-39', 'Wmo_40-49', 'Wmo_50-59', 'Wmo_60-69', 'Wmo_70-79', 'Wmo_80-89', 'Wmo_90-99', 'YearBuilt__under_1899', 'YearBuilt_1900-1909', 'YearBuilt_1910-1919', 'YearBuilt_1920-1929', 'YearBuilt_1930-1939', 'YearBuilt_1940-1949', 'YearBuilt_1950-1959', 'YearBuilt_1960-1969', 'YearBuilt_1970-1979', 'YearBuilt_1980-1989', 'YearBuilt_1990-1999', 'YearBuilt_2000-2009', 'YearBuilt_2010-2019', 'YearBuilt_2020-2029', 'Type_BUNGALOW', 'Type_COTTAGE', 'Type_DETACHED', 'Type_END OF TERRACE', 'Type_FLAT', 'Type_SEMI-DETACHED', 'Type_MID-TERRACE', 'NumRooms', 'NumOccupants', 'Total', 'AlwaysOn', 'Intermit', 'HVAC']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# file_path = '/content/allHouses.csv'\n",
    "file_path_Unfiltered = r'C:\\Users\\nicho\\OneDrive - The University of Western Ontario\\Ecolux\\Databases\\REFIT\\Combined Normalized\\allHousesUnfiltered.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "# house_df = pd.read_csv(file_path)\n",
    "predict_missing_data_house_df = pd.read_csv(file_path_Unfiltered)\n",
    "train_no_missing_data_house_df = predict_missing_data_house_df[predict_missing_data_house_df['Total'].notna()]\n",
    "missing_data_rows = predict_missing_data_house_df[predict_missing_data_house_df['Total'].isnull()]\n",
    "\n",
    "\n",
    "# print(house_df.columns.to_list())\n",
    "print(predict_missing_data_house_df.columns.to_list())\n",
    "print(train_no_missing_data_house_df.columns.to_list())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f4be37-50cb-4884-98d0-5433aff0db10",
   "metadata": {
    "id": "11f4be37-50cb-4884-98d0-5433aff0db10",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# Read in the csv\n",
    "houses_dataset = r'C:\\Users\\nicho\\OneDrive - The University of Western Ontario\\Ecolux\\Databases\\REFIT\\Combined Normalized\\allHousesUnfiltered.csv'\n",
    "house_df = pd.read_csv(houses_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gSPUk1disgdF",
   "metadata": {
    "id": "gSPUk1disgdF",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setting up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "866fabb2-03e7-4a8b-af53-d7f94a991443",
   "metadata": {
    "id": "866fabb2-03e7-4a8b-af53-d7f94a991443"
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import ElasticNet\n",
    "# from sklearn.multioutput import MultiOutputRegressor\n",
    "# from sklearn.preprocessing import PowerTransformer\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# print(\"house_df---->>>>\",house_df.columns.to_list())\n",
    "# selected_features = ['AlwaysOn', 'Intermit', 'HVAC',]\n",
    "# drop_columns_train = [ 'AlwaysOn', 'Intermit', 'HVAC', 'HouseNum', 'RealTemp', 'ApparTemp', 'Humid', 'Wmo_0-9', 'Wmo_10-19', 'Wmo_20-29', 'Wmo_30-39', 'Wmo_40-49', 'Wmo_50-59', 'Wmo_60-69', 'Wmo_70-79', 'Wmo_80-89', 'Wmo_90-99']\n",
    "# drop_columns_train.extend(selected_features)\n",
    "\n",
    "# X = house_df.drop(columns=drop_columns_train, axis=1)\n",
    "# Y = house_df[selected_features] # Last 3 are ['AlwaysOn', 'Intermit', 'HVAC']\n",
    "# # Assuming X is your feature matrix and Y is the target matrix with n columns\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "# y_test_array = y_test.values\n",
    "\n",
    "# # ######Uncomment the below code if you want to merge AlwaysOn and HVAC#########################\n",
    "# # Y = house_df[['AlwaysOn', 'Intermit', 'HVAC']].copy()  # Last 3 columns\n",
    "# # Y['Combined'] = Y['AlwaysOn'] + Y['HVAC']\n",
    "# # Y.drop(columns=['AlwaysOn', 'HVAC'], inplace=True)\n",
    "# # #########################################################################################\n",
    "\n",
    "# print(\"X.head----->>>>>\", X.columns.to_list())\n",
    "# print(\"Y.head----->>>>>\",Y.columns.to_list())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a13cce-2366-4c08-8fb2-2e4baba9e5fe",
   "metadata": {
    "id": "35a13cce-2366-4c08-8fb2-2e4baba9e5fe"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def inverse_yeo_johnson(y, lambda_):\n",
    "    if lambda_ == 0:\n",
    "        return np.exp(y) - 1\n",
    "    else:\n",
    "        if y >= 0:\n",
    "            return (y * lambda_ + 1) ** (1 / lambda_) - 1\n",
    "        else:\n",
    "            return -((1 - y * (2 - lambda_)) ** (1 / (2 - lambda_)) - 1)\n",
    "\n",
    "\n",
    "# must be passed in as a Dataframe\n",
    "# total_energy_inverseYeoJohnson is the transformed values |\n",
    "# total_energy_actual is the real values for energy\n",
    "def calculate_total_inverse_yeo_johnson(total_df, lambda_value=-0.06419593996677918):\n",
    "    if 'Total' in total_df.columns:\n",
    "        total_energy_inverseYeoJohnson = pd.DataFrame(total_df['Total'])\n",
    "        total_energy_actual = total_energy_inverseYeoJohnson.apply(lambda row: inverse_yeo_johnson(row['Total'], lambda_value), axis=1)\n",
    "        total_energy_actual_df = pd.DataFrame(total_energy_actual, columns=['Total'])\n",
    "        total_energy_actual_array = total_energy_actual.values.flatten()\n",
    "\n",
    "        print(\"Original total values, [Total]:\\n\", total_energy_inverseYeoJohnson.sort_index().head(3))\n",
    "        print(\"coverted total energy test data\\n\", total_energy_actual_df.sort_index().head(3))\n",
    "\n",
    "        return total_energy_actual, total_energy_actual_df, total_energy_actual_array\n",
    "    else:\n",
    "        print(\"No 'Total' column found in the DataFrame.\")\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def convert_fractions_to_actuals(fractions, total_energy_actual_array, columns_to_apply=['AlwaysOn', 'Intermit', 'HVAC']):\n",
    "    # Check if the required columns exist in fractions DataFrame\n",
    "    if isinstance(fractions, pd.DataFrame):\n",
    "        valid_columns = [col for col in columns_to_apply if col in fractions.columns]\n",
    "    else:\n",
    "        valid_columns = columns_to_apply\n",
    "        print(\"valid_columns\",valid_columns)\n",
    "\n",
    "    # If the required columns exist, apply the conversion\n",
    "    if valid_columns:\n",
    "        fractions_array = fractions[valid_columns].values if isinstance(fractions, pd.DataFrame) else fractions\n",
    "        fractions_actuals = fractions_array * total_energy_actual_array[:, np.newaxis]\n",
    "        if isinstance(fractions, pd.DataFrame):\n",
    "            fractions_actuals_df = pd.DataFrame(fractions_actuals, index=fractions.index, columns=valid_columns)\n",
    "            print(\"Result: fractions_actuals_df\\n\", fractions_actuals_df.sort_index().head(3))\n",
    "        else:\n",
    "            print(\"Result: fractions_actuals\\n\", fractions_actuals[:3])\n",
    "        return fractions_actuals, fractions_actuals_df if isinstance(fractions, pd.DataFrame) else None\n",
    "    else:\n",
    "        print(\"No valid columns found for conversion in ['AlwaysOn', 'Intermit', 'HVAC']\")\n",
    "        return None, None\n",
    "\n",
    "# Example usage:\n",
    "# y_test_actuals, y_test_actuals_df = convert_fractions_to_actuals(y_pred_gb, total_energy_actual_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Pt4iARtxVPJ_",
   "metadata": {
    "id": "Pt4iARtxVPJ_",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Performance Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9rA4f6-tBFq",
   "metadata": {
    "id": "e9rA4f6-tBFq"
   },
   "source": [
    "Definitions:<br>\n",
    "**MSE**:<br>\n",
    "-> Definition: MSE measures the average of\n",
    "the squared differences between actual and predicted values. It is calculated by taking the average of the square of the errors (difference between actual and predicted values). <br>\n",
    "-> Usage: MSE is useful for evaluating the variance of the model's errors. Lower MSE values indicate better model performance\n",
    "<br>\n",
    "**RMSE**:<br>\n",
    "-> Definition: RMSE is the square root of the mean squared error. It measures the standard deviation of the residuals (errors).<br>\n",
    "-> Usage: RMSE is more interpretable than MSE because it is in the same units as the target variable. It provides insight into how close the model's predictions are to the actual values.\n",
    "<br>\n",
    "**MAE**:<br>\n",
    "-> Definition: MAE measures the average of the absolute differences between actual and predicted values. It is calculated by taking the mean of the absolute differences between actual and predicted values.<br>\n",
    "-> Usage: MAE is useful for measuring the average magnitude of errors, without considering their direction. Lower MAE values indicate better model performance.\n",
    "<br>\n",
    "**MAPE**:<br>\n",
    "-> Definition: MAPE measures the average of the absolute percentage differences between actual and predicted values. It expresses errors as percentages of actual values.<br>\n",
    "-> Usage: MAPE provides insight into the relative size of errors. It is useful for comparing model performance across different scales.\n",
    "<br>\n",
    "**R^2**:<br>\n",
    "-> Definition: R^2 measures the proportion of the variance in the target variable that is explained by the model. It ranges from 0 to 1, where 1 indicates a perfect fit.<br>\n",
    "-> Usage: R^2 provides a measure of how well the model explains the variability of the target variable. Higher values indicate better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "puE_g4N_nd60",
   "metadata": {
    "id": "puE_g4N_nd60"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# #make sure you convert to the same values\n",
    "def calculate_performance_metrics(y_test_actuals, y_pred_actuals_model, y_test_fracs=None, y_pred_fracs_model=None):\n",
    "    # Calculate performance metrics\n",
    "    metrics = {}\n",
    "\n",
    "\n",
    "\n",
    "    metrics['mse'] = mean_squared_error(y_test_actuals, y_pred_actuals_model, multioutput='raw_values')\n",
    "    metrics['rmse'] = np.sqrt(metrics['mse'])\n",
    "    metrics['mae'] = mean_absolute_error(y_test_actuals, y_pred_actuals_model, multioutput='raw_values')\n",
    "    metrics['mape'] = mean_absolute_percentage_error(y_test_actuals, y_pred_actuals_model, multioutput='raw_values')\n",
    "    metrics['r2'] = r2_score(y_test_actuals, y_pred_actuals_model, multioutput='raw_values')\n",
    "\n",
    "    # Calculate differences and absolute differences\n",
    "    differences = y_pred_actuals_model - y_test_actuals\n",
    "    absolute_differences = np.abs(differences)\n",
    "\n",
    "    # Calculate additional metrics\n",
    "    metrics['max_difference'] = np.max(differences, axis=0)\n",
    "    metrics['mean_difference'] = np.mean(differences, axis=0)\n",
    "    metrics['median_difference'] = np.median(differences, axis=0)\n",
    "    metrics['mean_absolute_difference'] = np.mean(absolute_differences, axis=0)\n",
    "    metrics['median_absolute_difference'] = np.median(absolute_differences, axis=0)\n",
    "\n",
    "\n",
    "    if y_test_fracs is not None and y_pred_fracs_model is not None:\n",
    "        # Now doing it in fractions\n",
    "        absolute_differences_fracs = np.abs(y_test_fracs - y_pred_fracs_model)\n",
    "        metrics['mean_absolute_difference_fracs'] = np.mean(absolute_differences_fracs, axis=0)\n",
    "        metrics['max_absolute_difference_fracs'] = np.max(absolute_differences_fracs, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    # Check if any of the predicted values are negative\n",
    "    negative_values = y_pred_actuals_model < 0\n",
    "    num_negative_values = np.sum(negative_values)\n",
    "    if num_negative_values > 0:\n",
    "        print(f\"Warning!!: {num_negative_values} predicted values are negative\")\n",
    "    return metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def calculate_performance_metrics(y_test_actuals, y_pred_actuals_model, y_test_fracs=None, y_pred_fracs_model=None):\n",
    "#     # Calculate performance metrics\n",
    "#     metrics = {}\n",
    "\n",
    "#     # Flatten arrays if they are multi-dimensional\n",
    "#     y_test_actuals_flat = y_test_actuals.values.flatten() if isinstance(y_test_actuals, pd.DataFrame) else np.asarray(y_test_actuals).flatten()\n",
    "#     y_pred_actuals_model_flat = y_pred_actuals_model.values.flatten() if isinstance(y_pred_actuals_model, pd.DataFrame) else np.asarray(y_pred_actuals_model).flatten()\n",
    "\n",
    "#     print(\"Shape of y_test_actuals_flat:\", y_test_actuals_flat.shape)\n",
    "#     print(\"Shape of y_pred_actuals_model_flat:\", y_pred_actuals_model_flat.shape)\n",
    "\n",
    "#     metrics['mse'] = mean_squared_error(y_test_actuals_flat, y_pred_actuals_model_flat)\n",
    "#     metrics['rmse'] = np.sqrt(metrics['mse'])\n",
    "#     metrics['mae'] = mean_absolute_error(y_test_actuals_flat, y_pred_actuals_model_flat)\n",
    "#     metrics['mape'] = mean_absolute_percentage_error(y_test_actuals_flat, y_pred_actuals_model_flat)\n",
    "#     metrics['r2'] = r2_score(y_test_actuals_flat, y_pred_actuals_model_flat)\n",
    "\n",
    "#     # Calculate differences and absolute differences\n",
    "#     differences = y_pred_actuals_model_flat - y_test_actuals_flat\n",
    "#     absolute_differences = np.abs(differences)\n",
    "\n",
    "#     # Calculate additional metrics\n",
    "#     metrics['max_difference'] = np.max(differences)\n",
    "#     metrics['mean_difference'] = np.mean(differences)\n",
    "#     metrics['median_difference'] = np.median(differences)\n",
    "#     metrics['mean_absolute_difference'] = np.mean(absolute_differences)\n",
    "#     metrics['median_absolute_difference'] = np.median(absolute_differences)\n",
    "\n",
    "#     if y_test_fracs is not None and y_pred_fracs_model is not None:\n",
    "#         # Now doing it in fractions\n",
    "#         absolute_differences_fracs = np.abs(y_test_fracs - y_pred_fracs_model)\n",
    "#         metrics['mean_absolute_difference_fracs'] = np.mean(absolute_differences_fracs)\n",
    "#         metrics['max_absolute_difference_fracs'] = np.max(absolute_differences_fracs)\n",
    "\n",
    "#     # Check if any of the predicted values are negative\n",
    "#     negative_values = y_pred_actuals_model_flat < 0\n",
    "#     num_negative_values = np.sum(negative_values)\n",
    "#     if num_negative_values > 0:\n",
    "#         print(f\"Warning!!: {num_negative_values} predicted values are negative\")\n",
    "\n",
    "#     return metrics\n",
    "\n",
    "\n",
    "def print_performance_metrics(metrics):\n",
    "    print(\"Y.head: \", Y.columns.to_list())\n",
    "    print(\"MSE per output:\", metrics['mse'])\n",
    "    print(\"RMSE per output:\", metrics['rmse'])\n",
    "    print(\"MAE per output:\", metrics['mae'])\n",
    "    print(\"MAPE per output:\", metrics['mape'])\n",
    "    print(\"R^2 per output:\", metrics['r2'])\n",
    "    print(\"Maximum Difference:\", metrics['max_difference'])\n",
    "    print(\"Mean Difference:\", metrics['mean_difference'])\n",
    "    print(\"Median Difference:\", metrics['median_difference'])\n",
    "    print(\"Mean Absolute Difference:\", metrics['mean_absolute_difference'])\n",
    "    print(\"Median Absolute Difference:\", metrics['median_absolute_difference'])\n",
    "\n",
    "    if 'mean_absolute_difference_fracs' in metrics:\n",
    "        print(\"Mean Absolute Difference in percentage(%): \", metrics['mean_absolute_difference_fracs'])\n",
    "        print(\"Maximum Absolute Difference in percentage(%): \", metrics['max_absolute_difference_fracs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9eCHUC3ecTn7",
   "metadata": {
    "id": "9eCHUC3ecTn7"
   },
   "outputs": [],
   "source": [
    "# feature importances for each output\n",
    "def feature_importance(model):\n",
    "  for i, estimator in enumerate(model.estimators_):\n",
    "      print(f\"Feature Importances for Output {y_train.columns[i]}:\")\n",
    "      feature_importances = estimator.feature_importances_\n",
    "      feature_importances_list = [(X.columns[j], importance) for j, importance in enumerate(feature_importances)]\n",
    "      feature_importances_list_sorted = sorted(feature_importances_list, key=lambda x: x[1], reverse=True)\n",
    "      for feature, importance in feature_importances_list_sorted:\n",
    "          print(f\"{feature}: {importance}\")\n",
    "      print(\"___________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ha2H1HgasJuZ",
   "metadata": {
    "id": "Ha2H1HgasJuZ",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Gq2sD-JrGVoQ",
   "metadata": {
    "id": "Gq2sD-JrGVoQ"
   },
   "source": [
    "## Residuals against predicted values Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Lu1Mfu0LGGSg",
   "metadata": {
    "id": "Lu1Mfu0LGGSg"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def residual_vs_predicted_plot(y_test_actuals, y_pred_actuals_model):\n",
    "    y_test_actuals_df = pd.DataFrame(y_test_actuals, columns=y_test.columns)\n",
    "    y_pred_actuals_model_df = pd.DataFrame(y_pred_actuals_model, columns=y_test.columns)\n",
    "\n",
    "    # Calculate residuals\n",
    "    residuals = y_test_actuals_df - y_pred_actuals_model_df\n",
    "\n",
    "\n",
    "    # Plot residuals against predicted values\n",
    "    for column in residuals.columns:\n",
    "\n",
    "        current_predicted_values = y_pred_actuals_model[:, residuals.columns.get_loc(column)]\n",
    "        current_residuals = residuals[column]\n",
    "\n",
    "        # Create a scatter plot of residuals vs. predicted values\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(current_predicted_values, current_residuals)\n",
    "        plt.axhline(0, color='red', linestyle='--')\n",
    "        plt.xlabel('Predicted Values')\n",
    "        plt.ylabel('Residuals')\n",
    "        plt.title(f'Residuals vs. Predicted Values (Linear Model) - {column}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u-q0RT83LANA",
   "metadata": {
    "id": "u-q0RT83LANA"
   },
   "source": [
    "## Histogram of residuals Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8JHke9zgIDjJ",
   "metadata": {
    "id": "8JHke9zgIDjJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def histogram_residuals(y_test_actuals, y_pred_actuals_model):\n",
    "    y_test_actuals_df = pd.DataFrame(y_test_actuals, columns=y_test.columns)\n",
    "    y_pred_actuals_model_df = pd.DataFrame(y_pred_actuals_model, columns=y_test.columns)\n",
    "\n",
    "    # Calculate residuals\n",
    "    residuals = y_test_actuals_df - y_pred_actuals_model_df\n",
    "\n",
    "    # calculate range based on the data, ignoring the extreme x% of values\n",
    "    # adjust this based on the range, just so we can see the results better\n",
    "    lower_percentile = residuals.quantile(0.05)#0.00\n",
    "    upper_percentile = residuals.quantile(0.95)\n",
    "\n",
    "    # Plot histogram of residuals for each target variable\n",
    "    for column in residuals.columns:\n",
    "        current_residuals = residuals[column]\n",
    "\n",
    "        range_min = lower_percentile[column]\n",
    "        range_max = upper_percentile[column]\n",
    "\n",
    "        # Create histogram of residuals\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.hist(current_residuals, bins=100, range=(range_min, range_max)) # range=(-1000, 1000)\n",
    "        plt.xlabel('Residuals')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.title(f'Histogram of Residuals - {column}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruaTPZslaqVR",
   "metadata": {
    "id": "ruaTPZslaqVR"
   },
   "source": [
    "## Violin Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ZUfs3BERaufM",
   "metadata": {
    "id": "ZUfs3BERaufM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def violin_plot_residuals(y_test_actuals, y_pred_actuals_model, top_percentage=5):\n",
    "    y_test_actuals_df = pd.DataFrame(y_test_actuals, columns=y_test.columns)\n",
    "    y_pred_actuals_model_df = pd.DataFrame(y_pred_actuals_model, columns=y_test.columns)\n",
    "\n",
    "    residuals = y_test_actuals_df - y_pred_actuals_model_df\n",
    "    absolute_residuals = residuals.abs()\n",
    "\n",
    "    ####can also log the values so you dont have to get rid of the percentage\n",
    "    # log_absolute_residuals = np.log1p(absolute_residuals)  # Apply log transformation\n",
    "\n",
    "    #  top x% of absolute residuals for filtering out\n",
    "    top_percentile = absolute_residuals.quantile(1 - top_percentage / 100)\n",
    "\n",
    "    for column in absolute_residuals.columns:\n",
    "        current_residuals = absolute_residuals[column]\n",
    "\n",
    "        filtered_residuals = current_residuals[current_residuals <= top_percentile[column]]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.violinplot(filtered_residuals, widths=0.7)\n",
    "        plt.xlabel('Target Variable')\n",
    "        plt.ylabel(f'Absolute Residuals (Excluding Top {top_percentage}%)')\n",
    "        plt.title(f'Violin Plot of Absolute Residuals (Excluding Top {top_percentage}%) - {column}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Y1nlokK9oyea",
   "metadata": {
    "id": "Y1nlokK9oyea"
   },
   "source": [
    "## QQ Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "TtRcQviCqxzp",
   "metadata": {
    "id": "TtRcQviCqxzp"
   },
   "outputs": [],
   "source": [
    "# ###QQ plot###\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "def qq_plot(y_test_actuals, y_pred_actuals_model):\n",
    "    # Calculate residuals\n",
    "  residuals_linear = y_test_actuals - y_pred_actuals_model\n",
    "\n",
    "  # QQ plot of residuals\n",
    "  # Loop through each column in the residuals\n",
    "  for column in residuals_linear.columns:\n",
    "      # Extract the residuals for the current column\n",
    "      current_residuals = residuals_linear[column]\n",
    "\n",
    "      # Create a QQ plot for the current residuals\n",
    "      plt.figure(figsize=(8, 6))\n",
    "      stats.probplot(current_residuals, dist=\"norm\", plot=plt)\n",
    "      plt.title(f'QQ Plot of Residuals (Linear Model) - {column}')\n",
    "      plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MJnEwifALIDX",
   "metadata": {
    "id": "MJnEwifALIDX"
   },
   "source": [
    "## Display all Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7AONd5N_HEsF",
   "metadata": {
    "id": "7AONd5N_HEsF"
   },
   "outputs": [],
   "source": [
    "def display_all_graphs(y_test_actuals, y_pred_actuals_model):\n",
    "  residual_vs_predicted_plot(y_test_actuals, y_pred_actuals_model)\n",
    "  histogram_residuals(y_test_actuals, y_pred_actuals_model)\n",
    "  violin_plot_residuals(y_test_actuals, y_pred_actuals_model, top_percentage=5)\n",
    "  qq_plot(y_test_actuals, y_pred_actuals_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "V0ICKIFjkFiO",
   "metadata": {
    "id": "V0ICKIFjkFiO"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from sklearn.inspection import plot_partial_dependence\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Features for which you want to plot PDPs\n",
    "# features_to_plot = ['Feature1', 'Feature2', 'Feature3']  # Replace with the features you want to visualize\n",
    "\n",
    "# # Output variables to visualize (index corresponds to the order of columns in y_train)\n",
    "# target_indices = [0, 1, 2]  # HVAC_frac, AlwaysOn_frac, Intermit_frac\n",
    "\n",
    "# # Create PDPs\n",
    "# fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# plot_partial_dependence(\n",
    "#     estimator=linear_model,\n",
    "#     X=X_train,\n",
    "#     features=features_to_plot,\n",
    "#     target=target_indices,\n",
    "#     ax=ax,\n",
    "#     grid_resolution=50  # Number of points to use in grid\n",
    "# )\n",
    "\n",
    "# # Set titles for the plot\n",
    "# ax[0].set_title('HVAC_frac')\n",
    "# ax[1].set_title('AlwaysOn_frac')\n",
    "# ax[2].set_title('Intermit_frac')\n",
    "\n",
    "# # Show plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5MUo52f1kOxx",
   "metadata": {
    "id": "5MUo52f1kOxx"
   },
   "outputs": [],
   "source": [
    "# ###SHAP (Shapley Additive Explanations)###\n",
    "\n",
    "# import shap\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# # Initialize the SHAP explainer for your model\n",
    "# explainer = shap.Explainer(linear_model, X_train)\n",
    "\n",
    "# # Compute SHAP values for the test set\n",
    "# shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# # Create SHAP summary plot for each target variable\n",
    "# for i, target in enumerate(['HVAC', 'AlwaysOn', 'Intermit']):\n",
    "#     # Use shap.summary_plot for each target variable\n",
    "#     shap.summary_plot(shap_values[i], X_test, feature_names=selected_features, show=False)\n",
    "#     plt.title(f'SHAP Summary Plot for {target}')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "yJTByX8w0WWe",
   "metadata": {
    "id": "yJTByX8w0WWe"
   },
   "outputs": [],
   "source": [
    "# import shap\n",
    "# from xgboost import XGBRegressor\n",
    "# ##https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/model_agnostic/Multioutput%20Regression%20SHAP.html\n",
    "# ##https://towardsdatascience.com/explainable-ai-for-multiple-regression-2df70cfc9995\n",
    "# # # Create an explainer object with the XGBoost model\n",
    "# # explainer = shap.Explainer(gb_model)\n",
    "\n",
    "# # # Calculate SHAP values\n",
    "# # shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# # # Visualize the SHAP values\n",
    "# # shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "# explainer = shap.KernelExplainer(model=gb_model.predict, data=X.head(50), link=\"identity\")\n",
    "\n",
    "# # Set the index of the specific example to explain\n",
    "# X_idx = 0\n",
    "\n",
    "# shap_value_single = explainer.shap_values(X=X.iloc[X_idx : X_idx + 1, :], nsamples=100)\n",
    "\n",
    "# X.iloc[X_idx : X_idx + 1, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "upZZ387j0ZvE",
   "metadata": {
    "id": "upZZ387j0ZvE"
   },
   "outputs": [],
   "source": [
    "# from sklearn.inspection import plot_partial_dependence\n",
    "\n",
    "# def plot_partial_dependence(model, X, feature_names):\n",
    "#     # Plot partial dependence for each feature\n",
    "#     fig, axs = plot_partial_dependence(model, X, features=[i for i in range(len(feature_names))], feature_names=feature_names, grid_resolution=50)\n",
    "#     fig.suptitle('Partial Dependence Plots')\n",
    "#     plt.show()\n",
    "\n",
    "# # Usage\n",
    "# plot_partial_dependence(gb_model, X_test, X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "FpJp7REr0lju",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FpJp7REr0lju",
    "outputId": "4254dc53-18ac-47ff-d7f2-17f14007f852"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gb_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Get feature importances from the trained XGBoost model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m feature_importances_gb \u001b[38;5;241m=\u001b[39m \u001b[43mgb_model\u001b[49m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_booster()\u001b[38;5;241m.\u001b[39mget_score(importance_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(feature_importances_gb)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize a dictionary to store feature importance percentages for each predictor\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gb_model' is not defined"
     ]
    }
   ],
   "source": [
    "############NOT SURE ABOUT THIS##################\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Get feature importances from the trained XGBoost model\n",
    "feature_importances_gb = gb_model.estimators_[0].get_booster().get_score(importance_type='weight')\n",
    "print(feature_importances_gb)\n",
    "# Initialize a dictionary to store feature importance percentages for each predictor\n",
    "feature_importance_percentage_gb = {}\n",
    "\n",
    "# Calculate total importance for each predictor\n",
    "total_importance_gb = sum(feature_importances_gb.values())\n",
    "\n",
    "# Calculate importance percentages for each feature in each predictor\n",
    "for target_column in Y.columns:\n",
    "    feature_importance_percentage_gb[target_column] = {}\n",
    "    for feature, importance in feature_importances_gb.items():\n",
    "        feature_importance_percentage_gb[target_column][feature] = (importance / total_importance_gb) * 100\n",
    "\n",
    "print(feature_importance_percentage_gb)\n",
    "# Create separate graphs for each predictor\n",
    "for target_column, importance_dict in feature_importance_percentage_gb.items():\n",
    "    # Convert dictionary to DataFrame for sorting\n",
    "    importance_df = pd.DataFrame(list(importance_dict.items()), columns=['Feature', 'Importance'])\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Calculate importance percentages for display\n",
    "    importance_df['Importance (%)'] = (importance_df['Importance'] / importance_df['Importance'].sum()) * 100\n",
    "\n",
    "    # Plot feature importances for the current predictor\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "    for index, value in enumerate(importance_df['Importance']):\n",
    "        plt.text(value, index, f'{value:.2f}%', ha='left')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title(f'Feature Importance for Predictor: {target_column} (XGBoost)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XpyI4lP206es",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpyI4lP206es",
    "outputId": "190abb17-7053-4db4-d98c-56c10aa10b08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06022225 0.04543082 0.01268391 0.00680095 0.01490394 0.02559882\n",
      " 0.00913699 0.00711785 0.00807204 0.00539723 0.         0.\n",
      " 0.         0.         0.01049488 0.00917203 0.00487436 0.\n",
      " 0.         0.02512721 0.         0.         0.         0.\n",
      " 0.02212516 0.         0.05642403 0.01276595 0.02882228 0.04179651\n",
      " 0.04204712 0.01139974 0.         0.         0.         0.02189942\n",
      " 0.         0.         0.39627457 0.02705124 0.         0.\n",
      " 0.         0.         0.05626161 0.03809909]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_importances  = gb_model.estimators_[0].feature_importances_\n",
    "print(feature_importances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-qi9ZLVcsoQf",
   "metadata": {
    "id": "-qi9ZLVcsoQf",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a66868-e38e-4989-9afc-c30191e0def7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9a66868-e38e-4989-9afc-c30191e0def7",
    "outputId": "e53c52cb-21d6-4b3f-f896-39f5e19a9404"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        AlwaysOn  Intermit      HVAC\n",
      "162687  0.080576  0.089216  0.830208\n",
      "22801   0.190761  0.098911  0.710328\n",
      "203564  0.396688  0.030601  0.572711\n",
      "140524  0.000000  0.166675  0.833325\n",
      "129027  0.126301  0.595352  0.278347\n",
      "AlwaysOn    0\n",
      "Intermit    0\n",
      "HVAC        0\n",
      "dtype: int64\n",
      "y_pred_fracs_linear [[0.18243149 0.18373948 0.63382902]\n",
      " [0.18243149 0.18373948 0.63382902]\n",
      " [0.18243149 0.18373948 0.63382902]\n",
      " ...\n",
      " [0.18243149 0.18373948 0.63382902]\n",
      " [0.18243149 0.18373948 0.63382902]\n",
      " [0.18243149 0.18373948 0.63382902]]\n",
      "result:                  0           1           2\n",
      "0      113.542990  114.357066  394.486944\n",
      "1      131.349581  132.291326  456.353094\n",
      "2      122.128398  123.004030  424.315572\n",
      "3      121.128127  121.996586  420.840287\n",
      "4       15.342489   15.452491   53.305021\n",
      "...           ...         ...         ...\n",
      "44089   41.276402   41.572344  143.408253\n",
      "44090   25.677233   25.861332   89.211435\n",
      "44091  158.865358  160.004384  551.952258\n",
      "44092  109.892353  110.680255  381.803392\n",
      "44093   43.006400   43.314746  149.418854\n",
      "\n",
      "[44094 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Print the first five rows of y_train\n",
    "print(y_train.head())\n",
    "\n",
    "# Check for missing values in y_train\n",
    "print(y_train.isnull().sum())\n",
    "\n",
    "# Initialize and fit the model\n",
    "linear_model = MultiOutputRegressor(ElasticNet(random_state=42, positive=True)).fit(X_train, y_train)\n",
    "\n",
    "# Predicting the percentages\n",
    "y_pred_fracs_linear = linear_model.predict(X_test)\n",
    "print(\"y_pred_fracs_linear\", y_pred_fracs_linear)\n",
    "\n",
    "\n",
    "# Converting back to energy values\n",
    "y_pred_actuals_linear = y_pred_fracs_linear * total_energy_inverseYeoJohnson_array\n",
    "result_df = pd.DataFrame(y_pred_actuals_linear)\n",
    "print(\"result: \",result_df.sort_index())\n",
    "# performance analysis\n",
    "metrics = calculate_performance_metrics(y_test_actuals, y_pred_actuals_linear)\n",
    "print_performance_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TfiD96KCrnC5",
   "metadata": {
    "id": "TfiD96KCrnC5",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd00eae-954a-4941-a533-a206e3405b0c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abd00eae-954a-4941-a533-a206e3405b0c",
    "outputId": "62427ea7-fa3c-4d3f-9039-4714ab594d47"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3f15c4e2e5e1>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m ))\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Predict the fractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mfit_params_validated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m    217\u001b[0m             delayed(_fit_estimator)(\n\u001b[1;32m    218\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_validated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36m_fit_estimator\u001b[0;34m(estimator, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1761\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize and fit the model\n",
    "rf_model = MultiOutputRegressor(RandomForestRegressor(\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    n_estimators=100, #try 50 # was 100\n",
    "    # max_depth = 10, #default is none\n",
    "    # max_features=0.5, # Fewer features per split, less memory\n",
    "    random_state=42\n",
    "))\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the fractions\n",
    "y_pred_fracs_rf = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Converting back to energy values\n",
    "y_pred_actuals_rf = y_pred_fracs_rf * total_energy_inverseYeoJohnson_array\n",
    "# calculate metrics\n",
    "metrics_rf = calculate_performance_metrics(y_test_actuals, y_pred_actuals_rf, y_test.values, y_pred_fracs_rf)\n",
    "print_performance_metrics(metrics_rf)\n",
    "feature_importance(rf_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LsNTVjGRg93x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LsNTVjGRg93x",
    "outputId": "db22ca99-9bbf-44aa-90e9-c6f8dbe9ac96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances for Output AlwaysOn:\n",
      "Total: 0.38008454980440326\n",
      "HouseNum: 0.24006994099173046\n",
      "ApparTemp: 0.051235383701939724\n",
      "RealTemp: 0.042077171393807185\n",
      "Humid: 0.0334604848232994\n",
      "YearBuilt_1970-1979: 0.031811054564990233\n",
      "TimeCos: 0.029837624328665043\n",
      "TimeSin: 0.02641076536038575\n",
      "YearBuilt_2010-2019: 0.02601604022191428\n",
      "NumOccupants: 0.025400990995057156\n",
      "MonthSin: 0.023774091280774023\n",
      "MonthCos: 0.02298367685784954\n",
      "DayNumSin: 0.016548958071928608\n",
      "NumRooms: 0.013930940288731127\n",
      "DayNumCos: 0.01024242060901279\n",
      "YearBuilt__under_1899: 0.005822469358943641\n",
      "YearBuilt_1990-1999: 0.004651917228854004\n",
      "YearBuilt_1940-1949: 0.0043590728308699825\n",
      "Type_DETACHED: 0.0024932558471122564\n",
      "Wmo_50-59: 0.002110910191025118\n",
      "Wmo_0-9: 0.0019719711277550738\n",
      "Type_SEMI-DETACHED: 0.001955523440558492\n",
      "YearBuilt_1960-1969: 0.0011722041377406477\n",
      "Wmo_60-69: 0.0006421227715175748\n",
      "YearBuilt_2000-2009: 0.0003306158424413664\n",
      "Type_MID-TERRACE: 0.0003162856757780372\n",
      "Wmo_70-79: 0.00016122979635477386\n",
      "YearBuilt_1980-1989: 0.00012832845656032444\n",
      "Wmo_10-19: 0.0\n",
      "Wmo_20-29: 0.0\n",
      "Wmo_30-39: 0.0\n",
      "Wmo_40-49: 0.0\n",
      "Wmo_80-89: 0.0\n",
      "Wmo_90-99: 0.0\n",
      "YearBuilt_1900-1909: 0.0\n",
      "YearBuilt_1910-1919: 0.0\n",
      "YearBuilt_1920-1929: 0.0\n",
      "YearBuilt_1930-1939: 0.0\n",
      "YearBuilt_1950-1959: 0.0\n",
      "YearBuilt_2020-2029: 0.0\n",
      "Type_BUNGALOW: 0.0\n",
      "Type_COTTAGE: 0.0\n",
      "Type_END OF TERRACE: 0.0\n",
      "Type_FLAT: 0.0\n",
      "Type_STUDENT HALLS: 0.0\n",
      "Type_FACTORY: 0.0\n",
      "Type_OFFICE: 0.0\n",
      "Type_UNIVERSITY: 0.0\n",
      "___________________________________________________________________\n",
      "Feature Importances for Output Intermit:\n",
      "Total: 0.35547385578299034\n",
      "HouseNum: 0.08797402000311003\n",
      "ApparTemp: 0.06235664600105364\n",
      "TimeSin: 0.059172321505283544\n",
      "Humid: 0.05881578803283235\n",
      "TimeCos: 0.057959184687288154\n",
      "RealTemp: 0.05559276557987633\n",
      "MonthSin: 0.04690569395137386\n",
      "MonthCos: 0.03307509092607883\n",
      "DayNumSin: 0.030824899073866034\n",
      "NumOccupants: 0.029812012227851868\n",
      "DayNumCos: 0.017496581061277466\n",
      "YearBuilt_1940-1949: 0.016340346975854417\n",
      "YearBuilt_2010-2019: 0.01502263470854158\n",
      "NumRooms: 0.011971115229710328\n",
      "YearBuilt_2000-2009: 0.010509980956498943\n",
      "YearBuilt_1970-1979: 0.01011424318740568\n",
      "Type_SEMI-DETACHED: 0.008897920732425834\n",
      "Type_DETACHED: 0.006239382718330408\n",
      "YearBuilt_1990-1999: 0.005555339751872268\n",
      "YearBuilt_1960-1969: 0.003915344278006584\n",
      "Wmo_50-59: 0.003672175870105383\n",
      "Wmo_0-9: 0.0035171726135651664\n",
      "YearBuilt_1980-1989: 0.0026359074609411943\n",
      "Type_MID-TERRACE: 0.0025544477450202834\n",
      "YearBuilt__under_1899: 0.0021109559456776693\n",
      "Wmo_60-69: 0.0012386516269511892\n",
      "Wmo_70-79: 0.0002455213662105804\n",
      "Wmo_10-19: 0.0\n",
      "Wmo_20-29: 0.0\n",
      "Wmo_30-39: 0.0\n",
      "Wmo_40-49: 0.0\n",
      "Wmo_80-89: 0.0\n",
      "Wmo_90-99: 0.0\n",
      "YearBuilt_1900-1909: 0.0\n",
      "YearBuilt_1910-1919: 0.0\n",
      "YearBuilt_1920-1929: 0.0\n",
      "YearBuilt_1930-1939: 0.0\n",
      "YearBuilt_1950-1959: 0.0\n",
      "YearBuilt_2020-2029: 0.0\n",
      "Type_BUNGALOW: 0.0\n",
      "Type_COTTAGE: 0.0\n",
      "Type_END OF TERRACE: 0.0\n",
      "Type_FLAT: 0.0\n",
      "Type_STUDENT HALLS: 0.0\n",
      "Type_FACTORY: 0.0\n",
      "Type_OFFICE: 0.0\n",
      "Type_UNIVERSITY: 0.0\n",
      "___________________________________________________________________\n",
      "Feature Importances for Output HVAC:\n",
      "Total: 0.25772407752496224\n",
      "HouseNum: 0.16374289922571186\n",
      "ApparTemp: 0.07073166242404666\n",
      "TimeCos: 0.06521819181458449\n",
      "RealTemp: 0.06212582047430718\n",
      "Humid: 0.06167675441735552\n",
      "TimeSin: 0.05968418389266124\n",
      "MonthCos: 0.038694767415061455\n",
      "MonthSin: 0.03454081502887614\n",
      "DayNumSin: 0.032410309536508405\n",
      "NumRooms: 0.02832153641355514\n",
      "DayNumCos: 0.01869182440450863\n",
      "YearBuilt_1960-1969: 0.017119241548207282\n",
      "NumOccupants: 0.01670698605182254\n",
      "YearBuilt_1970-1979: 0.011825329984282828\n",
      "YearBuilt__under_1899: 0.01136729503154058\n",
      "YearBuilt_2010-2019: 0.009517987663471306\n",
      "YearBuilt_2000-2009: 0.007449473937974273\n",
      "Type_SEMI-DETACHED: 0.005660103015704216\n",
      "YearBuilt_1940-1949: 0.005510072303104397\n",
      "Type_DETACHED: 0.004524996050304257\n",
      "YearBuilt_1990-1999: 0.003940468246621945\n",
      "Wmo_0-9: 0.0038107455679494717\n",
      "Wmo_50-59: 0.0037893983550985294\n",
      "Type_MID-TERRACE: 0.0021734641106137594\n",
      "YearBuilt_1980-1989: 0.001543964615831588\n",
      "Wmo_60-69: 0.0012307154194758226\n",
      "Wmo_70-79: 0.00026691552585815585\n",
      "Wmo_10-19: 0.0\n",
      "Wmo_20-29: 0.0\n",
      "Wmo_30-39: 0.0\n",
      "Wmo_40-49: 0.0\n",
      "Wmo_80-89: 0.0\n",
      "Wmo_90-99: 0.0\n",
      "YearBuilt_1900-1909: 0.0\n",
      "YearBuilt_1910-1919: 0.0\n",
      "YearBuilt_1920-1929: 0.0\n",
      "YearBuilt_1930-1939: 0.0\n",
      "YearBuilt_1950-1959: 0.0\n",
      "YearBuilt_2020-2029: 0.0\n",
      "Type_BUNGALOW: 0.0\n",
      "Type_COTTAGE: 0.0\n",
      "Type_END OF TERRACE: 0.0\n",
      "Type_FLAT: 0.0\n",
      "Type_STUDENT HALLS: 0.0\n",
      "Type_FACTORY: 0.0\n",
      "Type_OFFICE: 0.0\n",
      "Type_UNIVERSITY: 0.0\n",
      "___________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_importance(rf_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z6wLHOJDvmX8",
   "metadata": {
    "id": "z6wLHOJDvmX8"
   },
   "source": [
    "## Random Forest Regressor GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bGN2K60BWp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47bGN2K60BWp",
    "outputId": "9ed05510-34df-4786-99f7-d0a2c067a4e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    2.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params_ ----> Random Forest: {'estimator__max_depth': 10, 'estimator__max_features': 'sqrt', 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 5, 'estimator__n_estimators': 50}\n",
      "best_rf_model:  MultiOutputRegressor(estimator=RandomForestRegressor(max_depth=10,\n",
      "                                                     max_features='sqrt',\n",
      "                                                     min_samples_split=5,\n",
      "                                                     n_estimators=50, n_jobs=-1,\n",
      "                                                     random_state=42,\n",
      "                                                     verbose=1))\n",
      "Y.head:  ['AlwaysOn', 'Intermit', 'HVAC']\n",
      "MSE per output: [ 1242.41646797 15396.27668929 15593.94347091]\n",
      "RMSE per output: [ 35.24792856 124.08173391 124.87571209]\n",
      "MAE per output: [17.47085687 53.0290313  56.23861582]\n",
      "MAPE per output: [2.29000413e+15 6.50091093e+15 4.57373418e+12]\n",
      "R^2 per output: [0.63798071 0.80127235 0.90585379]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "param_grid = {\n",
    "    'estimator__n_estimators': [50, 100],\n",
    "    'estimator__max_depth': [5, 10],\n",
    "    'estimator__min_samples_split': [2, 5],\n",
    "    'estimator__min_samples_leaf': [1, 2],\n",
    "    'estimator__max_features': ['sqrt'],\n",
    "}\n",
    "\n",
    "rf_model = MultiOutputRegressor(RandomForestRegressor(\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    n_estimators=100, #try 50 # was 100\n",
    "    # max_depth = 10, #default is none\n",
    "    # max_features=0.5, # Fewer features per split, less memory\n",
    "    random_state=42\n",
    "))\n",
    "\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    rf_model,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "best_params_ = grid_search_rf.best_params_\n",
    "\n",
    "print(\"best_params_ ----> Random Forest:\", best_params_)\n",
    "print(\"best_rf_model: \", best_rf_model)\n",
    "y_pred_fracs_best_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Converting back to energy values\n",
    "y_pred_actuals_rf = y_pred_fracs_rf * total_energy_inverseYeoJohnson_array\n",
    "# calculate metrics\n",
    "metrics_rf = calculate_performance_metrics(y_test_actuals, y_pred_actuals_rf, y_test.values, y_pred_fracs_rf)\n",
    "print_performance_metrics(metrics_rf)\n",
    "feature_importance(rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x1urNr9bvpwU",
   "metadata": {
    "id": "x1urNr9bvpwU"
   },
   "source": [
    "## Random Forest Regressor RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fOZ-D_feZJpq",
   "metadata": {
    "id": "fOZ-D_feZJpq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_dist_rf = {\n",
    "    'estimator__n_estimators': randint(50, 200),  # Number of trees in the forest\n",
    "    'estimator__max_depth': [None, 10, 20, 30, 40],  # Maximum depth of trees\n",
    "    'estimator__min_samples_split': randint(2, 10),  # Minimum samples for splitting a node\n",
    "    'estimator__min_samples_leaf': randint(1, 4),  # Minimum samples per leaf\n",
    "    # 'estimator__max_features': ['auto', 'sqrt', 'log2', 1.0],  # Number of features to consider for splitting\n",
    "    'estimator__max_features': [1.0],  # Number of features to consider for splitting\n",
    "}\n",
    "\n",
    "# param_dist_rf = {\n",
    "#     'estimator__n_estimators': [10,100,200],  # Number of trees in the forest\n",
    "#     'estimator__max_depth': [3,5,10,None],  # Maximum depth of trees\n",
    "#     'estimator__min_samples_split': [1,2,3],  # Minimum samples for splitting a node\n",
    "#     'estimator__min_samples_leaf': [1,2,3],  # Minimum samples per leaf\n",
    "#     # 'estimator__max_features': ['auto', 'sqrt', 'log2', 1.0],  # Number of features to consider for splitting\n",
    "#     'estimator__max_features': [1,3,5,7],  # Number of features to consider for splitting\n",
    "# }\n",
    "\n",
    "\n",
    "# Initialize and fit the model\n",
    "rf_model = MultiOutputRegressor(RandomForestRegressor(\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    n_estimators=100, #try 50 # was 100\n",
    "    # max_depth = 10, #default is none\n",
    "    # max_features=0.5, # Fewer features per split, less memory\n",
    "    random_state=42\n",
    "))\n",
    "# rf_model = MultiOutputRegressor(RandomForestRegressor(random_state=42))\n",
    "\n",
    "\n",
    "randomized_search_rf = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_dist_rf,\n",
    "    n_iter=10,  # Number of random combinations to try\n",
    "    scoring='neg_mean_squared_error',  # Metric to optimize\n",
    "    # n_jobs=-1,\n",
    "    cv=3,  # Number of cross-validation folds\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "randomized_search_rf.fit(X_train, y_train)\n",
    "print(\"best_params_ ---> RandomForestRegressor:\", randomized_search_rf.best_params_)\n",
    "print(\"best_rf_model---> RandomForestRegressor:\", randomized_search_rf.best_estimator_)\n",
    "best_rf_model = randomized_search_rf.best_estimator_\n",
    "y_pred_fracs_best_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "# Converting back to energy values\n",
    "y_pred_actuals_rf = y_pred_fracs_best_rf * total_energy_inverseYeoJohnson_array\n",
    "\n",
    "# calculate metrics\n",
    "metrics_rf = calculate_performance_metrics(y_test_actuals, y_pred_actuals_rf, y_test.values, y_pred_fracs_rf)\n",
    "print_performance_metrics(metrics_rf)\n",
    "feature_importance(rf_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pCkRlwOgr295",
   "metadata": {
    "id": "pCkRlwOgr295",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46af6598-7d18-4f15-92c5-fe0d739f5a94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46af6598-7d18-4f15-92c5-fe0d739f5a94",
    "outputId": "59019c88-4130-48cf-99b6-c08902bd49e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original total values, [Total]:\n",
      "        Total\n",
      "4   4.791903\n",
      "6   3.172128\n",
      "12  4.472217\n",
      "coverted total energy test data\n",
      "       Total\n",
      "4   305.909\n",
      "6    33.708\n",
      "12  193.716\n",
      "Result: \n",
      "     AlwaysOn  Intermit     HVAC\n",
      "4     90.820    30.854  184.235\n",
      "6      1.860     7.427   24.421\n",
      "12    36.354     1.000  156.362\n",
      "Result: \n",
      " [[ 44.61497819 151.04050512 378.59636154]\n",
      " [ 77.38000699 223.35594012 443.52273884]\n",
      " [115.56213776 133.69214591 412.26277297]]\n",
      "y_test_actuals [[ 58.713 175.314 388.36 ]\n",
      " [ 62.174 161.135 496.685]\n",
      " [145.402 162.104 361.942]\n",
      " ...\n",
      " [ 33.065   5.832 831.925]\n",
      " [ 80.992 108.85  412.534]\n",
      " [  4.275   0.879 230.586]]\n",
      "y_pred_gb [[0.07168366 0.2426794  0.60829735]\n",
      " [0.10747313 0.31021917 0.61600894]\n",
      " [0.17262302 0.19970505 0.61582494]\n",
      " ...\n",
      " [0.01642008 0.16067392 0.7702935 ]\n",
      " [0.08931672 0.14827508 0.7429296 ]\n",
      " [0.09887329 0.07477449 0.852897  ]]\n",
      "y_pred_gb_actuals [[ 44.61497819 151.04050512 378.59636154]\n",
      " [ 77.38000699 223.35594012 443.52273884]\n",
      " [115.56213776 133.69214591 412.26277297]\n",
      " ...\n",
      " [ 14.298968   139.91838117 670.78850379]\n",
      " [ 53.80224749  89.31734799 447.52294737]\n",
      " [ 23.30839054  17.62733799 201.06193604]]\n",
      "Warning!!: 1310 predicted values are negative\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mse': array([ 1426.60707544, 18152.41753463, 18960.05656136]),\n",
       " 'rmse': array([ 37.77045241, 134.73090787, 137.69552121]),\n",
       " 'mae': array([19.37895636, 59.18979136, 62.84503664]),\n",
       " 'mape': array([2.94833438e+15, 8.00051519e+15, 1.38723101e+13]),\n",
       " 'r2': array([0.58431066, 0.76569743, 0.88553137]),\n",
       " 'max_difference': array([ 849.62616732, 8097.20576449, 1814.12138398]),\n",
       " 'mean_difference': array([ 0.1822286 ,  0.15104255, -0.53810232]),\n",
       " 'median_difference': array([ 1.07607672,  1.38553172, -0.43515872]),\n",
       " 'mean_absolute_difference': array([19.37895636, 59.18979136, 62.84503664]),\n",
       " 'median_absolute_difference': array([12.08711861, 17.87882937, 23.73135681])}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Getting negative values. Look at changing the objective\n",
    "gb_model = MultiOutputRegressor(XGBRegressor(objective='reg:squarederror',verbosity=1, n_estimators=100, random_state=42)).fit(X_train, y_train)\n",
    "\n",
    "# Predict the fractions\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "# TEMP fixing negatives\n",
    "# y_pred_gb = np.maximum(y_pred_gb, 0)\n",
    "\n",
    "total_energy_actual, total_energy_actual_df, total_energy_actual_array = calculate_total_inverse_yeo_johnson(X_test)\n",
    "y_test_actuals, y_test_actuals_df = convert_fractions_to_actuals(y_test, total_energy_actual_array)\n",
    "y_pred_gb_actuals, y_test_actuals_df = convert_fractions_to_actuals(y_pred_gb, total_energy_actual_array)\n",
    "print(\"y_test_actuals\",y_test_actuals)\n",
    "print(\"y_pred_gb\",y_pred_gb)\n",
    "print(\"y_pred_gb_actuals\",y_pred_gb_actuals)\n",
    "\n",
    "calculate_performance_metrics(y_test_actuals, y_pred_gb_actuals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Uqqf1e5DvOWv",
   "metadata": {
    "id": "Uqqf1e5DvOWv"
   },
   "source": [
    "## Gradient Boosting GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fa5482-1c50-4481-b4dd-26118290311a",
   "metadata": {
    "id": "b9fa5482-1c50-4481-b4dd-26118290311a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_gb = {\n",
    "    'estimator__n_estimators': [50, 100],\n",
    "    'estimator__max_depth': [3, 6, 9],\n",
    "    'estimator__learning_rate': [0.1, 0.01],\n",
    "    'estimator__subsample': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Initialize the MultiOutputRegressor with XGBRegressor\n",
    "gb_model = MultiOutputRegressor(XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "\n",
    "# Grid search\n",
    "grid_search_gb = GridSearchCV(\n",
    "    estimator=gb_model,\n",
    "    param_grid=param_grid_gb,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search_gb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_gb_model = grid_search_gb.best_estimator_\n",
    "best_params_gb = grid_search_gb.best_params_\n",
    "\n",
    "print(\"best_params_ ----> Gradient Boosting:\", best_params_gb)\n",
    "print(\"best_gb_model: \", best_gb_model)\n",
    "\n",
    "# Predict using the best model\n",
    "y_pred_fracs_gb = best_gb_model.predict(X_test)\n",
    "\n",
    "# Converting back to energy values\n",
    "y_pred_actuals_gb = y_pred_fracs_gb * total_energy_inverseYeoJohnson_array\n",
    "\n",
    "# calculate metrics\n",
    "metrics_gb = calculate_performance_metrics(y_test_actuals, y_pred_actuals_gb, y_test.values, y_pred_fracs_gb)\n",
    "print_performance_metrics(metrics_gb)\n",
    "feature_importance(gb_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aWQMnh0VvYif",
   "metadata": {
    "id": "aWQMnh0VvYif"
   },
   "source": [
    "## Gradient Boosting RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FgSOAr1svbLc",
   "metadata": {
    "id": "FgSOAr1svbLc"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define the parameter distributions\n",
    "param_dist_gb = {\n",
    "    'estimator__n_estimators': randint(50, 200),\n",
    "    'estimator__max_depth': [3, 6, 9, None],\n",
    "    'estimator__learning_rate': uniform(0.01, 0.1),\n",
    "    'estimator__subsample': uniform(0.5, 0.5),\n",
    "}\n",
    "\n",
    "# Initialize the MultiOutputRegressor with XGBRegressor\n",
    "gb_model = MultiOutputRegressor(XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "\n",
    "# Randomized search\n",
    "randomized_search_gb = RandomizedSearchCV(\n",
    "    estimator=gb_model,\n",
    "    param_distributions=param_dist_gb,\n",
    "    n_iter=10,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42\n",
    ")\n",
    "randomized_search_gb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its parameters\n",
    "best_gb_model = randomized_search_gb.best_estimator_\n",
    "best_params_gb = randomized_search_gb.best_params_\n",
    "\n",
    "print(\"best_params_ ----> Gradient Boosting:\", best_params_gb)\n",
    "print(\"best_gb_model: \", best_gb_model)\n",
    "\n",
    "# Predict using the best model\n",
    "y_pred_fracs_gb = best_gb_model.predict(X_test)\n",
    "\n",
    "# Converting back to energy values\n",
    "y_pred_actuals_gb = y_pred_fracs_gb * total_energy_inverseYeoJohnson_array\n",
    "\n",
    "# calculate metrics\n",
    "metrics_gb = calculate_performance_metrics(y_test_actuals, y_pred_actuals_gb, y_test.values, y_pred_fracs_gb)\n",
    "print_performance_metrics(metrics_gb)\n",
    "feature_importance(gb_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3_rWiHvgCCj",
   "metadata": {
    "id": "a3_rWiHvgCCj"
   },
   "source": [
    "# Total->Hvac->intermit&alwaysOn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "GjOfkJPpeEi4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GjOfkJPpeEi4",
    "outputId": "47923561-9116-48cd-f28c-6010b990f99e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   39.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.head----->>>>> ['TimeSin', 'TimeCos', 'DayNumSin', 'DayNumCos', 'MonthSin', 'MonthCos', 'RealTemp', 'ApparTemp', 'Humid', 'Wmo_0-9', 'Wmo_10-19', 'Wmo_20-29', 'Wmo_30-39', 'Wmo_40-49', 'Wmo_50-59', 'Wmo_60-69', 'Wmo_70-79', 'Wmo_80-89', 'Wmo_90-99', 'YearBuilt__under_1899', 'YearBuilt_1900-1909', 'YearBuilt_1910-1919', 'YearBuilt_1920-1929', 'YearBuilt_1930-1939', 'YearBuilt_1940-1949', 'YearBuilt_1950-1959', 'YearBuilt_1960-1969', 'YearBuilt_1970-1979', 'YearBuilt_1980-1989', 'YearBuilt_1990-1999', 'YearBuilt_2000-2009', 'YearBuilt_2010-2019', 'YearBuilt_2020-2029', 'Type_BUNGALOW', 'Type_COTTAGE', 'Type_DETACHED', 'Type_END OF TERRACE', 'Type_FLAT', 'Type_SEMI-DETACHED', 'Type_MID-TERRACE', 'NumRooms', 'NumOccupants']\n",
      "Y.head----->>>>> ['Total']\n",
      "Filling Rows\n",
      "        Total  AlwaysOn  Intermit      HVAC\n",
      "693  3.497579  0.053732  0.001126  0.945142\n",
      "694       NaN       NaN       NaN       NaN\n",
      "697       NaN       NaN       NaN       NaN\n",
      "418       NaN       NaN       NaN       NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "C:\\Users\\nicho\\anaconda3\\envs\\Ecolux\\lib\\site-packages\\pandas\\core\\indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\Users\\nicho\\AppData\\Local\\Temp\\ipykernel_28836\\992165630.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_data_rows.loc[missing_data_rows.index, 'Total'] = predicted_values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Total  AlwaysOn  Intermit      HVAC\n",
      "693  3.497579  0.053732  0.001126  0.945142\n",
      "694  5.365264       NaN       NaN       NaN\n",
      "697  5.484461       NaN       NaN       NaN\n",
      "418  4.525870       NaN       NaN       NaN\n",
      "Saved model\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from joblib import dump\n",
    "\n",
    "selected_features = ['Total']\n",
    "drop_columns_train = [ 'Total', 'AlwaysOn', 'Intermit', 'HVAC', 'HouseNum']\n",
    "drop_columns_train.extend(selected_features)\n",
    "\n",
    "X = train_no_missing_data_house_df.drop(columns=drop_columns_train, axis=1)\n",
    "Y = train_no_missing_data_house_df[selected_features]\n",
    "\n",
    "\n",
    "# gradient Boost\n",
    "#model = MultiOutputRegressor(XGBRegressor(objective='reg:squarederror',verbosity=1, n_estimators=100, random_state=42))\n",
    "#Random Forest\n",
    "model = MultiOutputRegressor(RandomForestRegressor(verbose=1, n_jobs=-1, n_estimators=100,random_state=42))    # max_depth = 10, #default is none# max_features=0.5, # Fewer features per split, less memory\n",
    "model.fit(X, Y)\n",
    "\n",
    "print(\"X.head----->>>>>\", X.columns.to_list())\n",
    "print(\"Y.head----->>>>>\",Y.columns.to_list())\n",
    "\n",
    "\n",
    "####################Train, Test, And evaluate #######################\n",
    "# print('Performance Evaluation')\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Making a prediction\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred_df = pd.DataFrame(y_pred, index=X_test.index, columns=Y.columns)\n",
    "\n",
    "# # Actual values\n",
    "# y_pred_actuals, a, b = calculate_total_inverse_yeo_johnson(pd.DataFrame(y_pred_df))\n",
    "# ytest_actuals, , _ = calculate_total_inverse_yeo_johnson(pd.DataFrame(y_test))\n",
    "\n",
    "# # Finding metrics\n",
    "# metrics = calculate_performance_metrics(y_test_actuals.values, y_pred_actuals.values)\n",
    "# print_performance_metrics(metrics)\n",
    "# print()\n",
    "############################################################################\n",
    "\n",
    "\n",
    "#predicting the values and filling in\n",
    "print('Filling Rows')\n",
    "indices_to_print = [693, 694, 697, 418]\n",
    "totals_houses_df = predict_missing_data_house_df.copy()\n",
    "print(totals_houses_df.loc[indices_to_print, ['Total', 'AlwaysOn','Intermit','HVAC']])\n",
    "X_missing = missing_data_rows.drop(columns=drop_columns_train, axis=1)\n",
    "predicted_values = model.predict(X_missing).flatten()\n",
    "totals_houses_df.loc[missing_data_rows.index, 'Total'] = predicted_values\n",
    "missing_data_rows.loc[missing_data_rows.index, 'Total'] = predicted_values\n",
    "print(totals_houses_df.loc[indices_to_print, ['Total', 'AlwaysOn','Intermit','HVAC']])\n",
    "\n",
    "# Saving the model\n",
    "model_path = r'C:\\Users\\nicho\\OneDrive - The University of Western Ontario\\Ecolux\\Borealis AI\\Models\\TotalPrediction.joblib'\n",
    "dump(model, model_path)\n",
    "print('Saved model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "PSb1xaVvhDjp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PSb1xaVvhDjp",
    "outputId": "a7420dbf-19c4-4514-a565-7f398fca3863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totals_houses_df---->>>> ['HouseNum', 'TimeSin', 'TimeCos', 'DayNumSin', 'DayNumCos', 'MonthSin', 'MonthCos', 'RealTemp', 'ApparTemp', 'Humid', 'Wmo_0-9', 'Wmo_10-19', 'Wmo_20-29', 'Wmo_30-39', 'Wmo_40-49', 'Wmo_50-59', 'Wmo_60-69', 'Wmo_70-79', 'Wmo_80-89', 'Wmo_90-99', 'YearBuilt__under_1899', 'YearBuilt_1900-1909', 'YearBuilt_1910-1919', 'YearBuilt_1920-1929', 'YearBuilt_1930-1939', 'YearBuilt_1940-1949', 'YearBuilt_1950-1959', 'YearBuilt_1960-1969', 'YearBuilt_1970-1979', 'YearBuilt_1980-1989', 'YearBuilt_1990-1999', 'YearBuilt_2000-2009', 'YearBuilt_2010-2019', 'YearBuilt_2020-2029', 'Type_BUNGALOW', 'Type_COTTAGE', 'Type_DETACHED', 'Type_END OF TERRACE', 'Type_FLAT', 'Type_SEMI-DETACHED', 'Type_MID-TERRACE', 'NumRooms', 'NumOccupants', 'Total', 'AlwaysOn', 'Intermit', 'HVAC']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   46.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.head----->>>>> ['TimeSin', 'TimeCos', 'DayNumSin', 'DayNumCos', 'MonthSin', 'MonthCos', 'RealTemp', 'ApparTemp', 'Humid', 'Wmo_0-9', 'Wmo_10-19', 'Wmo_20-29', 'Wmo_30-39', 'Wmo_40-49', 'Wmo_50-59', 'Wmo_60-69', 'Wmo_70-79', 'Wmo_80-89', 'Wmo_90-99', 'YearBuilt__under_1899', 'YearBuilt_1900-1909', 'YearBuilt_1910-1919', 'YearBuilt_1920-1929', 'YearBuilt_1930-1939', 'YearBuilt_1940-1949', 'YearBuilt_1950-1959', 'YearBuilt_1960-1969', 'YearBuilt_1970-1979', 'YearBuilt_1980-1989', 'YearBuilt_1990-1999', 'YearBuilt_2000-2009', 'YearBuilt_2010-2019', 'YearBuilt_2020-2029', 'Type_BUNGALOW', 'Type_COTTAGE', 'Type_DETACHED', 'Type_END OF TERRACE', 'Type_FLAT', 'Type_SEMI-DETACHED', 'Type_MID-TERRACE', 'NumRooms', 'NumOccupants', 'Total']\n",
      "Y.head----->>>>> ['HVAC']\n",
      "        Total  AlwaysOn  Intermit      HVAC\n",
      "693  3.497579  0.053732  0.001126  0.945142\n",
      "694  5.365264       NaN       NaN       NaN\n",
      "697  5.484461       NaN       NaN       NaN\n",
      "418  4.525870       NaN       NaN       NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "C:\\Users\\nicho\\anaconda3\\envs\\Ecolux\\lib\\site-packages\\pandas\\core\\indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\Users\\nicho\\AppData\\Local\\Temp\\ipykernel_28836\\220147965.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_data_rows.loc[missing_data_rows.index, 'HVAC'] = predicted_values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Total  AlwaysOn  Intermit      HVAC\n",
      "693  3.497579  0.053732  0.001126  0.945142\n",
      "694  5.365264       NaN       NaN  0.888931\n",
      "697  5.484461       NaN       NaN  0.886179\n",
      "418  4.525870       NaN       NaN  0.663079\n",
      "Saved model\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"totals_houses_df---->>>>\",totals_houses_df.columns.to_list())\n",
    "selected_features = ['HVAC']\n",
    "drop_columns_train = [ 'AlwaysOn', 'Intermit', 'HVAC', 'HouseNum']\n",
    "drop_columns_train.extend(selected_features)\n",
    "\n",
    "X = train_no_missing_data_house_df.drop(columns=drop_columns_train, axis=1)\n",
    "Y = train_no_missing_data_house_df[selected_features] # Last 3 are ['AlwaysOn', 'Intermit', 'HVAC']\n",
    "\n",
    "# gradient Boost\n",
    "#model = MultiOutputRegressor(XGBRegressor(objective='reg:squarederror',verbosity=1, n_estimators=100, random_state=42))\n",
    "#Random Forest\n",
    "model = MultiOutputRegressor(RandomForestRegressor(verbose=1, n_jobs=-1, n_estimators=100,random_state=42))    # max_depth = 10, #default is none# max_features=0.5, # Fewer features per split, less memory\n",
    "model.fit(X, Y)\n",
    "\n",
    "print(\"X.head----->>>>>\", X.columns.to_list())\n",
    "print(\"Y.head----->>>>>\",Y.columns.to_list())\n",
    "\n",
    "\n",
    "\n",
    "####################Train, Test, And evaluate #######################\n",
    "# print('Performance Evaluation')\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# # Making a prediction\n",
    "# y_pred = model.predict(X_test)\n",
    "# y_pred_df = pd.DataFrame(y_pred, index=X_test.index, columns=Y.columns)\n",
    "\n",
    "# # Converting to actuals\n",
    "# x_total_actuals, a, x_total_actuals_array = calculate_total_inverse_yeo_johnson(pd.DataFrame(X_test))\n",
    "# x_total_actuals_array_reshaped = x_total_actuals_array.reshape(-1, 1)\n",
    "# y_pred_actuals = y_pred * x_total_actuals_array_reshaped\n",
    "# y_test_actuals = y_test * x_total_actuals_array_reshaped\n",
    "# y_test_actuals_reshaped = y_test_actuals.values.reshape(-1, 1)\n",
    "\n",
    "# metrics = calculate_performance_metrics(y_test_actuals_reshaped, y_pred_actuals)\n",
    "# print_performance_metrics(metrics)\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "# # predicting the values and filling in\n",
    "indices_to_print = [693, 694, 697, 418]\n",
    "HVAC_houses_df = totals_houses_df.copy()\n",
    "print(HVAC_houses_df.loc[indices_to_print, ['Total', 'AlwaysOn','Intermit','HVAC']])\n",
    "# missing_data_rows = predict_missing_data_house_df[predict_missing_data_house_df['Total'] == 0]\n",
    "X_missing = missing_data_rows.drop(columns=drop_columns_train, axis=1)\n",
    "predicted_values = model.predict(X_missing).flatten()\n",
    "HVAC_houses_df.loc[missing_data_rows.index, 'HVAC'] = predicted_values\n",
    "missing_data_rows.loc[missing_data_rows.index, 'HVAC'] = predicted_values\n",
    "print(HVAC_houses_df.loc[indices_to_print, ['Total', 'AlwaysOn','Intermit','HVAC']])\n",
    "\n",
    "# Saving the model\n",
    "model_path = r'C:\\Users\\nicho\\OneDrive - The University of Western Ontario\\Ecolux\\Borealis AI\\Models\\HVACPrediction.joblib'\n",
    "dump(model, model_path)\n",
    "print('Saved model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "KpgmCl44lw9q",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KpgmCl44lw9q",
    "outputId": "ca4d3f09-520a-4b08-e49c-b51eaa015a46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HVAC_houses_df---->>>> ['HouseNum', 'TimeSin', 'TimeCos', 'DayNumSin', 'DayNumCos', 'MonthSin', 'MonthCos', 'RealTemp', 'ApparTemp', 'Humid', 'Wmo_0-9', 'Wmo_10-19', 'Wmo_20-29', 'Wmo_30-39', 'Wmo_40-49', 'Wmo_50-59', 'Wmo_60-69', 'Wmo_70-79', 'Wmo_80-89', 'Wmo_90-99', 'YearBuilt__under_1899', 'YearBuilt_1900-1909', 'YearBuilt_1910-1919', 'YearBuilt_1920-1929', 'YearBuilt_1930-1939', 'YearBuilt_1940-1949', 'YearBuilt_1950-1959', 'YearBuilt_1960-1969', 'YearBuilt_1970-1979', 'YearBuilt_1980-1989', 'YearBuilt_1990-1999', 'YearBuilt_2000-2009', 'YearBuilt_2010-2019', 'YearBuilt_2020-2029', 'Type_BUNGALOW', 'Type_COTTAGE', 'Type_DETACHED', 'Type_END OF TERRACE', 'Type_FLAT', 'Type_SEMI-DETACHED', 'Type_MID-TERRACE', 'NumRooms', 'NumOccupants', 'Total', 'AlwaysOn', 'Intermit', 'HVAC']\n",
      "Intermit    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   45.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.head----->>>>> ['TimeSin', 'TimeCos', 'DayNumSin', 'DayNumCos', 'MonthSin', 'MonthCos', 'Wmo_0-9', 'Wmo_10-19', 'Wmo_20-29', 'Wmo_30-39', 'Wmo_40-49', 'Wmo_50-59', 'Wmo_60-69', 'Wmo_70-79', 'Wmo_80-89', 'Wmo_90-99', 'YearBuilt__under_1899', 'YearBuilt_1900-1909', 'YearBuilt_1910-1919', 'YearBuilt_1920-1929', 'YearBuilt_1930-1939', 'YearBuilt_1940-1949', 'YearBuilt_1950-1959', 'YearBuilt_1960-1969', 'YearBuilt_1970-1979', 'YearBuilt_1980-1989', 'YearBuilt_1990-1999', 'YearBuilt_2000-2009', 'YearBuilt_2010-2019', 'YearBuilt_2020-2029', 'Type_BUNGALOW', 'Type_COTTAGE', 'Type_DETACHED', 'Type_END OF TERRACE', 'Type_FLAT', 'Type_SEMI-DETACHED', 'Type_MID-TERRACE', 'NumRooms', 'NumOccupants', 'Total', 'HVAC']\n",
      "Y.head----->>>>> ['Intermit']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   15.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   43.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original total values, [Total]:\n",
      "        Total\n",
      "4   4.791903\n",
      "6   3.172128\n",
      "12  4.472217\n",
      "coverted total energy test data\n",
      "       Total\n",
      "4   305.909\n",
      "6    33.708\n",
      "12  193.716\n",
      "Y.head:  ['Intermit']\n",
      "MSE per output: [1127.8953638]\n",
      "RMSE per output: [33.58415346]\n",
      "MAE per output: [13.59601721]\n",
      "MAPE per output: [1.24854362e+15]\n",
      "R^2 per output: [0.98544168]\n",
      "Maximum Difference: [1025.65002619]\n",
      "Mean Difference: [-0.45557809]\n",
      "Median Difference: [0.15198652]\n",
      "Mean Absolute Difference: [13.59601721]\n",
      "Median Absolute Difference: [5.18631532]\n",
      "Feature Importances for Output Intermit:\n",
      "HVAC: 0.5047220726097289\n",
      "Total: 0.31775177260307946\n",
      "NumOccupants: 0.035496259566622175\n",
      "YearBuilt_2010-2019: 0.020943219248642925\n",
      "YearBuilt_1970-1979: 0.014322932930807606\n",
      "NumRooms: 0.012665828659436768\n",
      "YearBuilt_2000-2009: 0.012150004276871452\n",
      "MonthSin: 0.011771010779648079\n",
      "TimeSin: 0.009641949609273674\n",
      "TimeCos: 0.009550788627984879\n",
      "MonthCos: 0.008839219213745735\n",
      "YearBuilt_1940-1949: 0.00798238929090455\n",
      "Type_DETACHED: 0.007355809186986541\n",
      "YearBuilt_1990-1999: 0.006205154944132535\n",
      "DayNumSin: 0.005305598458670973\n",
      "Type_SEMI-DETACHED: 0.004533563253981187\n",
      "DayNumCos: 0.0034263686381275087\n",
      "YearBuilt__under_1899: 0.0021116771028363633\n",
      "YearBuilt_1960-1969: 0.001626487267269939\n",
      "Type_MID-TERRACE: 0.0013708794360076152\n",
      "Wmo_50-59: 0.000807414036384845\n",
      "Wmo_0-9: 0.0007759564193764991\n",
      "YearBuilt_1980-1989: 0.00028276591163179233\n",
      "Wmo_60-69: 0.00027754417266144163\n",
      "Wmo_70-79: 8.333375518664664e-05\n",
      "Wmo_10-19: 0.0\n",
      "Wmo_20-29: 0.0\n",
      "Wmo_30-39: 0.0\n",
      "Wmo_40-49: 0.0\n",
      "Wmo_80-89: 0.0\n",
      "Wmo_90-99: 0.0\n",
      "YearBuilt_1900-1909: 0.0\n",
      "YearBuilt_1910-1919: 0.0\n",
      "YearBuilt_1920-1929: 0.0\n",
      "YearBuilt_1930-1939: 0.0\n",
      "YearBuilt_1950-1959: 0.0\n",
      "YearBuilt_2020-2029: 0.0\n",
      "Type_BUNGALOW: 0.0\n",
      "Type_COTTAGE: 0.0\n",
      "Type_END OF TERRACE: 0.0\n",
      "Type_FLAT: 0.0\n",
      "___________________________________________________________________\n",
      "None\n",
      "        Total  AlwaysOn  Intermit      HVAC\n",
      "693  3.497579  0.053732  0.001126  0.945142\n",
      "694  5.365264       NaN       NaN  0.888931\n",
      "697  5.484461       NaN       NaN  0.886179\n",
      "418  4.525870       NaN       NaN  0.663079\n",
      "   HouseNum   TimeSin   TimeCos  DayNumSin  DayNumCos  MonthSin  MonthCos  \\\n",
      "0       1.0 -0.500000 -0.866025   0.974928  -0.222521 -0.866025       0.5   \n",
      "1       1.0 -0.707107 -0.707107   0.974928  -0.222521 -0.866025       0.5   \n",
      "2       1.0 -0.866025 -0.500000   0.974928  -0.222521 -0.866025       0.5   \n",
      "\n",
      "   RealTemp  ApparTemp  Humid  ...  Type_END OF TERRACE  Type_FLAT  \\\n",
      "0  0.792136   0.392199   0.59  ...                  0.0        0.0   \n",
      "1  0.607125   0.255182   0.65  ...                  0.0        0.0   \n",
      "2  0.311106   0.011597   0.70  ...                  0.0        0.0   \n",
      "\n",
      "   Type_SEMI-DETACHED  Type_MID-TERRACE  NumRooms  NumOccupants     Total  \\\n",
      "0                 0.0               0.0       0.4           0.2  5.489313   \n",
      "1                 0.0               0.0       0.4           0.2  5.301124   \n",
      "2                 0.0               0.0       0.4           0.2  5.240370   \n",
      "\n",
      "   AlwaysOn  Intermit      HVAC  \n",
      "0  0.041201  0.001036  0.957763  \n",
      "1  0.095364  0.001536  0.903100  \n",
      "2  0.152939  0.001684  0.845377  \n",
      "\n",
      "[3 rows x 47 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "C:\\Users\\nicho\\anaconda3\\envs\\Ecolux\\lib\\site-packages\\pandas\\core\\indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\Users\\nicho\\AppData\\Local\\Temp\\ipykernel_28836\\1015366444.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  missing_data_rows.loc[missing_data_rows.index, 'HVAC'] = predicted_values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Total  AlwaysOn  Intermit      HVAC\n",
      "693  3.497579  0.053732  0.001126  0.945142\n",
      "694  5.365264       NaN       NaN  0.888931\n",
      "697  5.484461       NaN       NaN  0.886179\n",
      "418  4.525870       NaN       NaN  0.663079\n",
      "Saved model\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"HVAC_houses_df---->>>>\",HVAC_houses_df.columns.to_list())\n",
    "selected_features = ['Intermit']\n",
    "drop_columns_train = [ 'AlwaysOn', 'Intermit', 'HouseNum', 'RealTemp', 'ApparTemp', 'Humid'] # , 'Wmo_0-9', 'Wmo_10-19', 'Wmo_20-29', 'Wmo_30-39', 'Wmo_40-49', 'Wmo_50-59', 'Wmo_60-69', 'Wmo_70-79', 'Wmo_80-89', 'Wmo_90-99'\n",
    "drop_columns_train.extend(selected_features)\n",
    "\n",
    "X = train_no_missing_data_house_df.drop(columns=drop_columns_train, axis=1)\n",
    "Y = train_no_missing_data_house_df[selected_features] # Last 3 are ['AlwaysOn', 'Intermit', 'HVAC']\n",
    "\n",
    "# gradient Boost\n",
    "#model = MultiOutputRegressor(XGBRegressor(objective='reg:squarederror',verbosity=1, n_estimators=100, random_state=42))\n",
    "#Random Forest\n",
    "model = MultiOutputRegressor(RandomForestRegressor(verbose=1, n_jobs=-1, n_estimators=100,random_state=42))    # max_depth = 10, #default is none# max_features=0.5, # Fewer features per split, less memory\n",
    "model.fit(X, Y)\n",
    "\n",
    "print(\"X.head----->>>>>\", X.columns.to_list())\n",
    "print(\"Y.head----->>>>>\",Y.columns.to_list())\n",
    "\n",
    "\n",
    "####################Train, Test, And evaluate #######################\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making the predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_df = pd.DataFrame(y_pred, index=X_test.index, columns=Y.columns)\n",
    "\n",
    "# Converting to actuals\n",
    "x_total_actuals, a, x_total_actuals_array = calculate_total_inverse_yeo_johnson(pd.DataFrame(X_test))\n",
    "x_total_actuals_array_reshaped = x_total_actuals_array.reshape(-1, 1)\n",
    "y_pred_actuals = y_pred * x_total_actuals_array_reshaped\n",
    "y_test_actuals = y_test * x_total_actuals_array_reshaped\n",
    "y_test_actuals_reshaped = y_test_actuals.values.reshape(-1, 1)\n",
    "\n",
    "# Calculating metrics\n",
    "metrics = calculate_performance_metrics(y_test_actuals_reshaped, y_pred_actuals)\n",
    "print_performance_metrics(metrics)\n",
    "print(feature_importance(model))\n",
    "############################################################################\n",
    "\n",
    "# predicting the values and filling in\n",
    "indices_to_print = [693, 694, 697, 418]\n",
    "Intermit_houses_df = HVAC_houses_df.copy()\n",
    "print(Intermit_houses_df.loc[indices_to_print, ['Total', 'AlwaysOn','Intermit','HVAC']])\n",
    "print(Intermit_houses_df.head(3))\n",
    "# missing_data_rows = predict_missing_data_house_df[predict_missing_data_house_df['Total'] == 0]\n",
    "X_missing = missing_data_rows.drop(columns=drop_columns_train, axis=1)\n",
    "predicted_values = model.predict(X_missing).flatten()\n",
    "Intermit_houses_df.loc[missing_data_rows.index, 'Intermit'] = predicted_values\n",
    "missing_data_rows.loc[missing_data_rows.index, 'HVAC'] = predicted_values\n",
    "print(HVAC_houses_df.loc[indices_to_print, ['Total', 'AlwaysOn','Intermit','HVAC']])\n",
    "\n",
    "# Saving the model\n",
    "model_path = r'C:\\Users\\nicho\\OneDrive - The University of Western Ontario\\Ecolux\\Borealis AI\\Models\\IntermitPrediction.joblib'\n",
    "dump(model, model_path)\n",
    "print('Saved model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "22ae12d8-bf24-4b7a-a1c3-89cdd9ec4d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   HouseNum   TimeSin   TimeCos  DayNumSin  DayNumCos  MonthSin  MonthCos  \\\n",
      "0       1.0 -0.500000 -0.866025   0.974928  -0.222521 -0.866025       0.5   \n",
      "1       1.0 -0.707107 -0.707107   0.974928  -0.222521 -0.866025       0.5   \n",
      "2       1.0 -0.866025 -0.500000   0.974928  -0.222521 -0.866025       0.5   \n",
      "\n",
      "   RealTemp  ApparTemp  Humid  ...  Type_END OF TERRACE  Type_FLAT  \\\n",
      "0  0.792136   0.392199   0.59  ...                  0.0        0.0   \n",
      "1  0.607125   0.255182   0.65  ...                  0.0        0.0   \n",
      "2  0.311106   0.011597   0.70  ...                  0.0        0.0   \n",
      "\n",
      "   Type_SEMI-DETACHED  Type_MID-TERRACE  NumRooms  NumOccupants     Total  \\\n",
      "0                 0.0               0.0       0.4           0.2  5.489313   \n",
      "1                 0.0               0.0       0.4           0.2  5.301124   \n",
      "2                 0.0               0.0       0.4           0.2  5.240370   \n",
      "\n",
      "   AlwaysOn  Intermit      HVAC  \n",
      "0  0.041201  0.001036  0.957763  \n",
      "1  0.095364  0.001536  0.903100  \n",
      "2  0.152939  0.001684  0.845377  \n",
      "\n",
      "[3 rows x 47 columns]\n",
      "        Total  AlwaysOn  Intermit      HVAC\n",
      "693  3.497579  0.053732  0.001126  0.945142\n",
      "694  5.365264  0.089623  0.021445  0.888931\n",
      "697  5.484461  0.080846  0.032975  0.886179\n",
      "418  4.525870  0.331176  0.005745  0.663079\n"
     ]
    }
   ],
   "source": [
    "Intermit_houses_df.loc[missing_data_rows.index, 'AlwaysOn'] = 1.0 - Intermit_houses_df.loc[missing_data_rows.index, 'Intermit'] - Intermit_houses_df.loc[missing_data_rows.index, 'HVAC']\n",
    "print(Intermit_houses_df.head(3))\n",
    "print(Intermit_houses_df.loc[indices_to_print, ['Total', 'AlwaysOn','Intermit','HVAC']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "59fbeb0c-2b91-4100-b8f3-0e5c413aa143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made the dataset\n"
     ]
    }
   ],
   "source": [
    "filled_path = r'C:\\Users\\nicho\\OneDrive - The University of Western Ontario\\Ecolux\\Databases\\REFIT\\Combined Normalized\\allHousesFilled.csv'\n",
    "\n",
    "Intermit_houses_df.to_csv(filled_path, index=False)\n",
    "print('Made the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "44620a97-29d2-44ec-a90f-eea33388298a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Total, AlwaysOn, Intermit, HVAC]\n",
      "Index: []\n",
      "\n",
      "Empty DataFrame\n",
      "Columns: [Total, AlwaysOn, Intermit, HVAC]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# # Applying the softmax function to get percentages that equal 100%\n",
    "# from scipy.special import softmax\n",
    "\n",
    "# # Only apply to the ones where AlwaysOn is negative\n",
    "# negative_alwayson_indices = Intermit_houses_df[Intermit_houses_df['AlwaysOn'] < 0].index\n",
    "\n",
    "# print(Intermit_houses_df.loc[negative_alwayson_indices, ['Total', 'AlwaysOn','Intermit','HVAC']])\n",
    "# print()\n",
    "\n",
    "# indices_to_apply_softmax = negative_alwayson_indices.intersection(missing_data_rows.index)\n",
    "# data_to_softmax = Intermit_houses_df.loc[indices_to_apply_softmax, ['AlwaysOn', 'Intermit', 'HVAC']]\n",
    "\n",
    "# # Apply softmax\n",
    "# softmax_results = softmax(data_to_softmax, axis=1)\n",
    "\n",
    "# # Update\n",
    "# Intermit_houses_df.loc[indices_to_apply_softmax, ['AlwaysOn', 'Intermit', 'HVAC']] = softmax_results\n",
    "\n",
    "# print(Intermit_houses_df.loc[negative_alwayson_indices, ['Total', 'AlwaysOn','Intermit','HVAC']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "LSGTcUSbhMkD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LSGTcUSbhMkD",
    "outputId": "c6a734c9-296b-4ce5-9d55-c1463a537a89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HVAC_houses_df---->>>> ['HouseNum', 'TimeSin', 'TimeCos', 'DayNumSin', 'DayNumCos', 'MonthSin', 'MonthCos', 'RealTemp', 'ApparTemp', 'Humid', 'YearBuilt__under_1899', 'YearBuilt_1900-1909', 'YearBuilt_1910-1919', 'YearBuilt_1920-1929', 'YearBuilt_1930-1939', 'YearBuilt_1940-1949', 'YearBuilt_1950-1959', 'YearBuilt_1960-1969', 'YearBuilt_1970-1979', 'YearBuilt_1980-1989', 'YearBuilt_1990-1999', 'YearBuilt_2000-2009', 'YearBuilt_2010-2019', 'YearBuilt_2020-2029', 'Type_BUNGALOW', 'Type_COTTAGE', 'Type_DETACHED', 'Type_END OF TERRACE', 'Type_FLAT', 'Type_SEMI-DETACHED', 'Type_MID-TERRACE', 'NumRooms', 'NumOccupants', 'Total', 'AlwaysOn', 'Intermit', 'HVAC']\n",
      "X.head----->>>>> ['TimeSin', 'TimeCos', 'DayNumSin', 'DayNumCos', 'MonthSin', 'MonthCos', 'YearBuilt__under_1899', 'YearBuilt_1900-1909', 'YearBuilt_1910-1919', 'YearBuilt_1920-1929', 'YearBuilt_1930-1939', 'YearBuilt_1940-1949', 'YearBuilt_1950-1959', 'YearBuilt_1960-1969', 'YearBuilt_1970-1979', 'YearBuilt_1980-1989', 'YearBuilt_1990-1999', 'YearBuilt_2000-2009', 'YearBuilt_2010-2019', 'YearBuilt_2020-2029', 'Type_BUNGALOW', 'Type_COTTAGE', 'Type_DETACHED', 'Type_END OF TERRACE', 'Type_FLAT', 'Type_SEMI-DETACHED', 'Type_MID-TERRACE', 'NumRooms', 'NumOccupants', 'Total', 'HVAC']\n",
      "Y.head----->>>>> ['AlwaysOn', 'Intermit']\n",
      "        Total  AlwaysOn  Intermit      HVAC\n",
      "693  3.497579  0.053732  0.001126  0.945142\n",
      "694  5.160087  0.000000  0.000000  0.791986\n",
      "697  5.423553  0.000000  0.000000  0.652282\n",
      "418  4.617801  0.000000  0.000000  0.714743\n",
      "        Total  AlwaysOn  Intermit      HVAC\n",
      "693  3.497579  0.053732  0.001126  0.945142\n",
      "694  5.160087  0.672605  0.089870  0.791986\n",
      "697  5.423553  0.525049  0.117528  0.652282\n",
      "418  4.617801  0.443642  0.302183  0.714743\n"
     ]
    }
   ],
   "source": [
    "#####################I dont think this will work\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"HVAC_houses_df---->>>>\",HVAC_houses_df.columns.to_list())\n",
    "selected_features = ['AlwaysOn', 'Intermit']\n",
    "drop_columns_train = [ 'AlwaysOn', 'Intermit', 'HouseNum', 'RealTemp', 'ApparTemp', 'Humid'] # 'Wmo_0-9', 'Wmo_10-19', 'Wmo_20-29', 'Wmo_30-39', 'Wmo_40-49', 'Wmo_50-59', 'Wmo_60-69', 'Wmo_70-79', 'Wmo_80-89', 'Wmo_90-99'\n",
    "drop_columns_train.extend(selected_features)\n",
    "\n",
    "X = HVAC_houses_df.drop(columns=drop_columns_train, axis=1)\n",
    "Y = HVAC_houses_df[selected_features] # Last 3 are ['AlwaysOn', 'Intermit', 'HVAC']\n",
    "\n",
    "\n",
    "# gradient Boost\n",
    "model = MultiOutputRegressor(XGBRegressor(objective='reg:squarederror',verbosity=1, n_estimators=100, random_state=42))\n",
    "#Random Forest\n",
    "# model = MultiOutputRegressor(RandomForestRegressor(verbose=1, n_jobs=-1, n_estimators=100,random_state=42))    # max_depth = 10, #default is none# max_features=0.5, # Fewer features per split, less memory\n",
    "model.fit(X, Y)\n",
    "\n",
    "print(\"X.head----->>>>>\", X.columns.to_list())\n",
    "print(\"Y.head----->>>>>\",Y.columns.to_list())\n",
    "\n",
    "####################Train, Test, And evaluate #######################\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "# model.fit(X_train, y_train)\n",
    "# y_pred_gb = model.predict(X_test)\n",
    "# y_pred_gb_df = pd.DataFrame(y_pred_gb, index=X_test.index, columns=Y.columns)\n",
    "\n",
    "# y_pred_actuals, a, b = calculate_total_inverse_yeo_johnson(pd.DataFrame(y_pred_gb_df))\n",
    "# y_test_actuals, _, _ = calculate_total_inverse_yeo_johnson(pd.DataFrame(y_test))\n",
    "# metrics = calculate_performance_metrics(y_test_actuals.values, y_pred_actuals.values)\n",
    "# print_performance_metrics(metrics)\n",
    "# y_pred_fracs_gb = gb_model.predict(X_test)\n",
    "############################################################################\n",
    "\n",
    "\n",
    "\n",
    "#####################WARNING, look at the reshape is it correct? chance of mismatch?\n",
    "\n",
    "# predicting the values and filling in\n",
    "indices_to_print = [693, 694, 697, 418]\n",
    "AlwaysOnIntermit_features_df = HVAC_houses_df.copy()\n",
    "print(AlwaysOnIntermit_features_df.loc[indices_to_print, ['Total', 'AlwaysOn','Intermit','HVAC']])\n",
    "# missing_data_rows = predict_missing_data_house_df[predict_missing_data_house_df['Total'] == 0]\n",
    "X_missing = missing_data_rows.drop(columns=drop_columns_train, axis=1)\n",
    "predicted_values = model.predict(X_missing).flatten()\n",
    "predicted_values = predicted_values.reshape(27304, 2)\n",
    "AlwaysOnIntermit_features_df.loc[missing_data_rows.index, ['AlwaysOn', 'Intermit']] = predicted_values\n",
    "# Extracting predicted values for 'AlwaysOn' and 'Intermit'\n",
    "# Filling in the predicted values for 'AlwaysOn' and 'Intermit' columns\n",
    "\n",
    "print(AlwaysOnIntermit_features_df.loc[indices_to_print, ['Total', 'AlwaysOn','Intermit','HVAC']])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# AlwaysOnIntermit_features_df = HVAC_houses_df.copy()# add pred vals to df\n",
    "# empty_indices = AlwaysOnIntermit_features_df[AlwaysOnIntermit_features_df['AlwaysOn'].isnull() | HVAC_houses_df['Intermit'].isnull()].index\n",
    "# missing_X = X.loc[empty_indices]\n",
    "# predicted_values = gb_model.predict(missing_X)\n",
    "# AlwaysOnIntermit_features_df.loc[empty_indices, ['AlwaysOn', 'Intermit']] = predicted_values\n",
    "# print(AlwaysOnIntermit_features_df.head(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UB0OKtp-oW8e",
   "metadata": {
    "id": "UB0OKtp-oW8e"
   },
   "source": [
    "# Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rPNUIH25obxa",
   "metadata": {
    "id": "rPNUIH25obxa"
   },
   "outputs": [],
   "source": [
    "\n",
    "#find empty and take out\n",
    "missing_data = house_df.isnull().any(axis=1)\n",
    "missing_features = house_df[missing_data].iloc[:, :-4]  # takes out last x columns 4=['Total', 'AlwaysOn', 'Intermit', 'HVAC']\n",
    "\n",
    "# predict missing values using model\n",
    "predicted_values = rf_model.predict(missing_features)\n",
    "\n",
    "\n",
    "training_columns =  ['Total', 'AlwaysOn', 'Intermit', 'HVAC']\n",
    "\n",
    "# replace missing values with predicted values\n",
    "filled_houses_df = house_df.copy()\n",
    "missing_indices = filled_houses_df.index[missing_data]\n",
    "filled_houses_df.loc[missing_indices, training_columns] = predicted_values\n",
    "\n",
    "#uncomment to convert for better viewing\n",
    "# filled_houses_df.to_csv('filled_Houses.csv', index=False)\n",
    "\n",
    "\n",
    "# TODO: convert verythin back? yeojohnson and shit and calc fractions? or is this not neccassary?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IOyhyy50-4Bw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IOyhyy50-4Bw",
    "outputId": "0a6e7fa2-273d-4001-dab5-57828ae98184"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_columns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5c1bc200f96d>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust the number of columns based on the number of features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Plot histogram or density plot for predicted values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'blue'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Predicted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_columns' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot histograms or density plots for the predicted values and corresponding non-missing values\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(16, 4))  # Adjust the number of columns based on the number of features\n",
    "\n",
    "for i, column in enumerate(training_columns):\n",
    "    # Plot histogram or density plot for predicted values\n",
    "    axes[i].hist(predicted_values[:, i], bins=30, alpha=0.5, color='blue', label='Predicted')\n",
    "\n",
    "    # Plot histogram or density plot for corresponding non-missing values\n",
    "    axes[i].hist(house_df.loc[~missing_data, column], bins=30, alpha=0.5, color='green', label='Actual')\n",
    "\n",
    "    # Set plot title and labels\n",
    "    axes[i].set_title(f'Distribution of {column}')\n",
    "    axes[i].set_xlabel(column)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "    # Add legend\n",
    "    axes[i].legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Idz7K8g4_owU",
   "metadata": {
    "id": "Idz7K8g4_owU"
   },
   "source": [
    "# PART 2 Womp Womp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d457LwnH_n6l",
   "metadata": {
    "id": "d457LwnH_n6l"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
